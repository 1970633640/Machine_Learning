# 模型评估与选择

## 误差

在分类任务中，通常把错分的样本数占样本总数的比例称为**错误率（error rate）**。比如m个样本有a个预测错了，错误率就是`a/m`；与错误率相对的有**精度（accuracy）**，或者说正确率，数值上等于1-错误率。

更一般地，通常会把模型输出和真实值之间的差异称为**误差（error）**。在训练集上的误差称为**训练误差（training error）**或者**经验误差（empirical error）**。而在新样本上的误差则称为**泛化误差（generalization error）**。我们希望模型的泛化误差尽可能小，但现实是，我们无法知道新样本是怎样的，所以只能尽可能地利用训练数据来最小化经验误差。

但是否经验误差小，泛化误差就一定小呢？这不是一定的，如果模型相比训练数据来说过于复杂，那就很有可能把训练数据本身的一些特点当作整个样本空间的特点，从而使得在训练数据上有很小的经验误差，但一旦面对新样本就会有很大误差，这种情况叫做**过拟合（overfitting）**。相对的是**欠拟合（underfitting）**。

欠拟合很容易避免，只要适当地增加模型复杂度（比方说增加神经网络的层数）就好。但**过拟合是无法彻底避免的**，只能缓解（减少模型复杂度/增加训练数据），这也是机器学习发展中的一个关键阻碍。

在现实任务中，要处理一个问题，我们往往有多种算法可以选择，即使是同一个算法也需要进行参数的选择，这就是机器学习中的**模型选择（model selection）**问题。既然泛化误差无法使用，而经验误差又存在着过拟合问题，不适合作为标准，那么我们应该如何进行模型选择呢？针对这个问题，后面的三个小节会给出回答。

这里先简单归纳一下，书中将模型选择问题拆解为（1）评估方法；（2）性能度量；（3）比较检验；三个子问题。可以这样理解：

- **评估方法**：用什么数据做评估？如何获得这些数据？

- **性能度量**：评估时如何衡量模型的好坏？有哪些评价标准？

- **比较检验**：如何比较模型的性能？注意不是简单地比大小！在机器学习中性能比较是相当复杂的。

## 评估方法

前面已经提到了不能把经验误差用作模型评估，否则会存在过拟合的嫌疑。那么很自然地，我们就会想到是否有一种方法能近似泛化误差呢？答案是有的，就是使用**测试集（testing set）**进行评估，利用**测试误差（testing error）**来近似泛化误差。

测试集和训练集一样，从样本空间中独立同分布采样而得，并且应尽可能与训练集互斥，也即用于训练的样本不应再出现在测试集中，否则就会高估模型的性能。为什么呢？举个例子，老师布置了2道题做课后作业，如果考试还是出这2两题，只能证明大家记住了这2道题；只有出不一样的题，才能看出大家是否真的掌握了知识，具备了举一反三的能力。

**注意**！！测试数据更多地是指模型在实际使用中遇到的数据，为了和模型评估中使用的测试集进行区分，一般会把模型评估用的测试集叫做**验证集（validation set）**。举个例子，在Kaggle或者天池上参加比赛，我们一般会拿到一份带标记的原始数据集和一份不带标记的测试数据集。我们需要选用一种评估方法来把原始数据集划分成训练集和验证集，然后进行训练，并按照模型在验证集上的性能表现来进行选择。最后挑出最好的模型对测试集的样本进行预测，并提交预测结果。下文将介绍几种常用的评估方法。

#### 留出法

直接将数据集划分为两个互斥集合，注意保持数据分布的一致性（比如比例相似）。保留类别比例的采样方式又叫**分层采样（stratified sampling）**。举个例子，原始数据集有100个样本，假设训练集占70个，验证集占30个。若训练集中正例反例各35个，也即比例为`1:1`，那么验证集中就应该正例反例个15个，同样保持`1:1`的比例。当然，这个比例最好还是遵循原始数据集中数据的分布规律。

单独一次留出法的结果往往不可靠，一般是进行多次随机划分，然后取各次评估的平均值作为评估结果。

留出法最大的缺点就是要进行划分，当训练集占的比例较大时，模型可以更准确地刻画原始数据集的特征，但是因为验证集较小，评估的结果往往不稳定也不准确；当训练集占的比例较小时，训练出的模型又不能充分学习到原始数据集的特征，评估结果可信度不高。这个问题没有完美的解决方案，一般取数据集2/3~4/5的样本作为训练集，余下的作为验证集。

#### 交叉验证

又称为**k折交叉验证（k-fold cross validation）**，将数据集划分为k个互斥子集。每次使用k-1个子集的并集作为训练集，余下的一个子集作为验证集，这就构成了k组训练/验证集，从而可以进行k次训练和验证。最终取k次验证的均值作为评估结果。
常用的k值包括5，10，20。

类似于留出法，因为存在多种划分k个子集的方式，为了减少因不同的样本划分而引入的差别，需要进行多次k折交叉验证。例如10次10折交叉验证，指的是进行了总计100次训练和100次评估。

特别地，令k=数据集样本数的交叉验证称为**留一法（Leave-One-Out，简称LOO）**，即有多少样本就进行多少次训练/验证，并且每次只留下一个样本做验证。这样做的好处是不需要担心随即样本划分带来的误差，因为这样的划分是唯一的。一般来说，留一法的评估结果被认为是比较准确的。但是！当数据集较大时，使用留一法需要训练的模型太多了！这种计算开销是难以忍受的！

#### 自助法

在留出法和交叉验证法中，我们都需要对数据集进行划分，从而使得训练所用的数据集比源数据集小，引入了一些因规模不同而造成的偏差，有没有办法避免规模不同造成的影响呢？

**自助法（bootstrapping）**正是我们需要的答案，以**自助采样（bootstrap sampling）**为基础，对包含m个样本的源数据集进行有放回的m次采样以获得同等规模的训练集。在这m次采样中都不被抽到的概率大约为0.368，也即源数据集中有大约1/3的样本是训练集中没有的。因此，我们可以采用这部分样本作为验证集，所得的结果称为**包外估计（out-of-bag estimate）**。


**注意**，自助法适用于数据集小，难以划分训练/验证集的情况。因为自助法能产生多个不同训练集，所以对集成学习也大有好处。但是！**自助法改变了数据集的分布**，也因此引入了一些额外的误差。因此，数据量足的时候还是留出法和交叉验证法用得多一些。

#### 调参和最终模型

**调参（parameter tuning）**一般先选定一个范围和变化步长，比如(0,1]，步长0.2，这样就有五个参数候选值。然后进行评估，选出最好的一个。这样选出的未必是全局最优的参数，但为了在开销和性能之间折中，只能这么做，毕竟我们无法试尽参数的所有取值。而且多个参数组合的情况是指数上升的，比方说有3个参数，每个参数评估5种取值，就需要测试多达 $5^3$ 种情形。

**特别注意**，训练/验证这个过程是为了让我们**确定学习算法和算法的参数**，确定了这些之后，我们需要再利用整个源数据集进行训练，这次训练所得的模型才是最终模型，也即提交给用户，进行测试的模型。

## 性能度量

**性能度量（performance measure）**指的是用于衡量模型泛化能力的评价标准。使用不同的性能度量往往导致不同的评判结果。比方说搭建推荐系统，两个模型中一个精度高，一个覆盖度高，如果我们想让更多的商品得到推荐可以就会选后一个模型。所以说，模型的好坏是相对的，取决于我们采用什么性能度量，而**采用什么性能度量则应取决于我们的任务需求**。

这个小节主要介绍分类任务中常用的性能度量。

#### 错误率和精度

在本章的开头已经提及到了，不再累述，这两个性能度量可写作更一般的形式，基于数据分布和概率密度函数进行定义。

#### 查准率，查全率，F1

假设我们正常处理一个二分类问题，按照模型预测值和真实值可以把测试样本划分为四种情形：**真正例（true positive），假正例（false positive），真反例（true negative），假反例（false negative）**。可以把结果表示为下图这个矩阵——**混淆矩阵(confusion matrix)**。

<table>

<tr>
    <th rowspan="2" align="center">真实情况</th>
    <th colspan="2" align="center">预测结果</th>
</tr>

<tr>
    <td align="center">正例</td>
    <td align="center">反例</td>
</tr>
<tr>
    <td align="center">正例</td>
    <td align="center">TP（真正例）</td>
    <td align="center">FN（假反例）</td>
</tr>
<tr>
    <td align="center">反例</td>
    <td align="center">FP（假正例）</td>
    <td align="center">TN（真反例）</td>
</tr>
</table>


**查准率，又称准确率（precision）**，用于衡量模型避免错误的能力，分母是模型预测的正例数目。

$$precision = \frac{TP}{TP+FP}$$

**查全率，又称召回率（recall）**，用于衡量模型避免缺漏的能力，分母是测试样本真正包含的正例数目。

$$recall = \frac{TP}{TP+FN}$$

**F1**，是查准率和查全率的调和平均，用于综合考虑这两个性能度量。

$$\frac{1}{F1} = \frac{1}{2} \times (\frac{1}{precision} + \frac{1}{recall}) \Rightarrow F1 = \frac{2 \times precision \times recall}{presion + recall}$$

有时候我们对查准率，查全率的需求是不同的。比方说广告推荐，要尽量避免打扰用户，因此查准率更重要；而逃犯检索，因为漏检的危害很大，所以查全率更重要。这时就需要使用$F_\beta$了。

**$F_\beta$**，是查准率和查全率的加权调和平均，用于综合考虑这两个性能度量，并采用不同的权重。

$$\frac{1}{F_\beta} = \frac{1}{1+\beta^2} \times (\frac{1}{precision} + \frac{\beta^2}{recall}) \Rightarrow F_\beta = \frac{(1+\beta^2) \times precision \times recall}{(\beta^2 \times presion) + recall}$$

其中 $\beta>0$ 度量了查全率对查准率的相对重要性，等于1时$F_\beta$退化为F1，小于1时查准率更重要，大于1时查全率更重要。

书中还介绍了如何对多次训练/测试产生的多个混淆矩阵进行评估，包括宏方法（先分别计算性能度量，再计算均值）和微方法（先对混淆矩阵各元素计算均值，再基于均值计算性能度量）两种途径。

#### ROC与AUC

很多时候，使用模型对测试样本进行预测得到的是一个实值或者概率（比如神经网络），需要进一步设置**阈值（threshold）**，然后把预测值和阈值进行比较才能获得最终预测的标记。

我们可以按照预测值对所有测试样本进行排序，最可能是正例的排前面，最不能是正例的排后面。这样分类时就像是在这个序列中以某个**截断点（cut point）**把样本分成两部分。我们需要**根据任务需求来设置截断点**。比如广告推荐更重视查准率，可能就会把截断点设置得更靠前。

因此！**排序本身的质量很能体现出一个模型的泛化性能**，ROC曲线就是一个用来衡量排序质量的工具。

**ROC，全称受试者工作特征（Receiver Operating Characteristic）**。怎样画ROC曲线呢？先定义两个重要的计算量：**真正例率（True Positive Rate，简称TPR）**和**假正例率（False Positive Rate，简称FPR）**。

$$TPR = \frac{TP}{TP+FN}$$

$$FPR = \frac{FP}{TP+FN}$$

TPR其实就等于召回率。在绘制ROC曲线时，纵轴为TPR，横轴为FPR。首先按预测值对样本进行排序，然后按序逐个把样本预测为正例，并计算此时的TPR和FPR，然后在图上画出该点，并与前一个点连线。如下图：

![ROC curve](http://www.unc.edu/courses/2010fall/ecol/563/001/images/lectures/lecture22/fig4.png)

有两个值得注意的特例：

- 经过 (0,1) 点的曲线，这代表所有正例都在反例之前出现（否则会先出现假正例从而无法经过 (0,1) 点），这是一个**理想模型**，我们可以设置一个阈值，完美地分割开正例和反例。

- 对角线，这对应于**随机猜测**模型，可以理解为真正例和假正例轮换出现，即每预测对一次接下来就预测错一次，可以看作是随机猜测的结果。

若一个模型的ROC曲线完全包住了另一个模型的ROC曲线，我们就认为这个模型更优。但是如果两条曲线发生交叉，要怎么判断呢？比较合理的判据是**AUC（Area Under ROC Curve）**，即ROC曲线下的面积。

$$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})$$

补充一点，ROC曲线上的面积等于**排序损失（loss）**。也即有：

$$AUC = 1 - \ell_{rank}$$

#### 代价敏感错误率与代价曲线

现实任务中，有时会遇到不同类型错误造成后果不同的状况。比如医生误诊，把患者诊断为健康人的影响远大于把健康人诊断为患者，因为可能因为这次误诊丧失了最佳治疗时机。为了权衡不同类型错误带来的不同损失，可以为这些错误类型赋以**非均等代价（unequal cost）**。

还是举二分类为类，可以**根据任务的领域知识**来设定一个**代价矩阵（cost matrix）**:

<table>

<tr>
    <th rowspan="2" align="center">真实类别</th>
    <th colspan="2" align="center">预测类别</th>
</tr>

<tr>
    <td align="center">第0类</td>
    <td align="center">第1类</td>
</tr>
<tr>
    <td align="center">第0类</td>
    <td align="center">0</td>
    <td align="center">$cost_{01}$</td>
</tr>
<tr>
    <td align="center">第1类</td>
    <td align="center">$cost_{10}$</td>
    <td align="center">0</td>
</tr>
</table>

预测值与真实值相等时，自然错误代价为0。但把第0类错预测为第1类和把第1类错预测为第0类这两种错误的代价是不同的。注意，**重要的不是代价在数值上的大小，而是它们的比值**。比方说 $\frac{cost_{01}}{cost_{10}} > 1$， 这就说明把第0类错预测为第1类的代价更高。

使用了非均等代价之后，我们在使用性能度量时自然也需要作出相应的改变，比方说**代价敏感（cost-sensitive）**版本的错误率：

$$E(f;D;cost) = \frac{1}{m}(\sum_{x_i \in D^+}I(f(x_i) \neq y_i) \times cost_{01} + \sum_{x_i \in D^-}I(f(x_i) \neq y_i) \times cost_{10})$$

由于ROC曲线不能反应使用非均等代价之后的期望总体代价，所以改用**代价曲线（cost curve）**来取替。
