# 线性模型

给定一个包含d个属性的实例 $\mathbf{x} = (x_1;x_2;...;x_d)$，**线性模型（linear model）**的原理是学得一个可以通过属性的线性组合来进行预测的函数，也即：

$$f(\mathbf{x}) = w_1x_1 + w_2x_2 + ... + w_dx_x + b$$

一般写作向量形式：$f(\mathbf{x}) = \mathbf{w}^T\mathbf{x} + b$。其中权重向量 $\mathbf{w}$ 和偏置项 $b$ 就是我们需要学习的参数。

线性模型有良好的可解释性，每个属性对应的权重可以理解为它对预测的重要性。并且建模较为简单，许多功能更为强大的非线性模型都是在线性模型的基础上引入层级结构或高维映射得到的。

这一章的内容大致如下：

- **线性回归**：如何把离散属性连续化？怎样用最小二乘法进行参数估计？多元线性回归如何求解？广义线性模型是怎样的？。

- **对数几率回归**：分类任务和线性回归是如何关联起来的？从概率的角度来看，如何用极大似然法进行参数估计并获取最优解？

- **线性判别分析**：二分类任务如何求得LDA模型的参数？如何推广到多分类任务？

- **多分类学习**：如何把多分类任务拆分为二分类任务？有哪些拆分策略？是如何进行建模和预测的？

- **类别不平衡问题**：再缩放思想以及三种解决类别不平衡问题的主要方法。

## 线性回归

#### 离散属性连续化

由于不同模型对数据的要求不一样，在建模之前，我们需要对数据做相应的处理。一般的线性回归模型要求属性的数据类型为连续值，故需要对离散属性进行连续化。

具体分两种情况：

1. 属性值之间有序：也即属性值有明确的大小关系，比方说把三值属性 “高度” 的取值 {高，中，低} 转换（编码）为 {1.0，0.5，0.0}；

2. 属性值之间无序：若该属性有 $k$ 个属性值，则把它转换为 $k$ 维向量（把1个属性扩展为k个属性），比方说把无序离散属性 “商品” 的取值 {牙膏，牙刷，毛巾} 转换为 (0,0,1)，(0,1,0)，(1,0,0)。 这种做法在自然语言处理和推荐系统实现中很常见，属性 “单词” 和 “商品” 都是无序离散变量，在建模前往往需要把这样的变量转换为[哑变量](http://baike.baidu.com/link?url=K78QE2bn2kp9p1kHmNT2iwrFQruUbpXASH6P-Ug4nHdKLTRNTTKnVVsvGCOpnQ0d8WfCqFJFW_xWaBRjbTi4Q-Mim_y0TiNwOVdMzgfFuPguT_a6wyu4g_Bmw2rup2MY0jTHYomPerh-vruZ6FNfLo3CpWhekBTolTyUG6DaTYe)，**否则会引入不恰当的序关系，从而影响后续处理**（比如距离的计算）。

**补充**：对应于离散属性连续化，自然也有**连续属性离散化**。比方说决策树建模就需要将连续属性离散化。此外，在作图观察数据分布特征时，往往也需要对连续属性进行离散化处理（比方说画直方图）。

#### 最小二乘法

回归任务最常用的性能度量是**均方误差（mean squared error, MSE）**。首先介绍**单变量线性回归**，试想我们要在二维平面上拟合一条曲线，则每个样例（即每个点）只包含一个实值属性（x值）和一个实值输出标记（y值），此时均方误差可定义为：

$$E(f;D) = \frac{1}{m} \sum_{i=1}^m(y_i-f(x_i))^2\\
\qquad\qquad = \frac{1}{m} \sum_{i=1}^m(y_i-wx_i-b)^2$$

有时我们会把这样描述模型总误差的式子称为**损失函数**或者**目标函数**（当该式是优化目标的时候）。这个函数的自变量是模型的参数 $w$ 和 $b$。由于给定训练集时，样本数 $m$ 是一个确定值，也即常数，所以可以把 $\frac{1}{m}$ 这一项拿走。

**最小二乘法（least square method）**就是基于均方误差最小化来进行模型求解的一种方法，寻找可使损失函数值最小的参数 $w$ 和 $b$ 的过程称为最小二乘**参数估计（parameter estimation）**。

通过对损失函数分别求参数 $w$ 和 $b$ 的偏导，并且令导数为0，可以得到这两个参数的**闭式（closed-form）解**（也即**解析解**）：

$$w = \frac{\sum_{i=1}^m y_i(x_i - \bar{x})}{\sum_{i=1}^m x_i^2 - \frac{1}{m}(\sum_{i=1}^m x_i)^2}$$

$$b = \frac{1}{m} \sum_{i=1}^m (y_i-wx_i)$$

在实际任务中，只要我们把自变量（x, y, m）的值代入就可以求出数值解了。

为什么可以这样求解呢？因为损失函数是一个**凸函数**（记住是向下凸，类似U型曲线），导数为0表示该函数曲线最低的一点，此时对应的参数值就是能使均方误差最小的参数值。特别地，**要判断一个函数是否凸函数，可以求其二阶导数**，若二阶导数在区间上非负则称其为凸函数，若在区间上恒大于零则称其为**严格凸函数**。

#### 多元线性回归

前面是直线拟合，样例只有一个属性。对于样例包含多个属性的情况，我们就要用到**多元线性回归（multivariate linear regression）**（又称作多变量线性回归）了。

令 $\mathbf{\hat{w}} = (\mathbf{w};b)$。把数据集表示为 $m \times (d+1)$ 大小的矩阵，每一行对应一个样例，前 $d$ 列是样例的 $d$ 个属性，**最后一列恒置为1**，对应偏置项。把样例的实值标记也写作向量形式，记作 $\mathbf{y}$。则此时损失函数为：

$$E_{\mathbf{\hat{w}}} = (\mathbf{y} - X\mathbf{\hat{w}})^T (\mathbf{y} - X\mathbf{\hat{w}})$$

同样使用最小二乘法进行参数估计，首先对 $\mathbf{\hat{w}}$ 求导：

$$\frac{\partial E_{\mathbf{\hat{w}}}}{\partial \mathbf{\hat{w}}} = 2 X^T(X\mathbf{\hat{w}} - \mathbf{y})$$

令该式值为0可得到 $\mathbf{\hat{w}}$ 的闭式解：

$$\mathbf{\hat{w}}* = (X^TX)^{-1}X^T\mathbf{y}$$

这就要求 $X^TX$ 必须是可逆矩阵，也即必须是**满秩矩阵（full-rank matrix）**，这是线性代数方面的知识，书中并未展开讨论。但是！**现实任务中 $X^TX$ 往往不是满秩的**，很多时候 $X$ 的列数很多，甚至超出行数（例如推荐系统，商品数是远远超出用户数的），此时 $X^TX$ 显然不满秩，会解出多个 $\mathbf{\hat{w}}$。这些解都能使得均方误差最小化，这时就需要由学习算法的**归纳偏好**决定了，常见的做法是引入**正则化（regularization）**项。

#### 广义线性模型

除了直接让模型预测值逼近实值标记 $y$，我们还可以让它逼近 $y$ 的衍生物，这就是**广义线性模型（generalized linear model）**的思想，也即：

$$y = g^{-1}(\mathbf{w^Tx} + b)$$

其中 $g(\cdot)$ 称为**联系函数（link function）**，使用广义线性模型我们可以实现强大的**非线性函数映射**功能。比方说**对数线性回归（log-linear regression）**，令 $g(\cdot) = ln(\cdot)$，此时模型预测值对应的是**实值标记在指数尺度上的变化**：

$$\ln y = \mathbf{w^Tx} + b$$
