# 神经网络

在机器学习中，**神经网络（neural networks）**一般是指“神经网络学习”。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由**具有适应性的简单单元**组成的广泛**并行互连的网络**，它的组织能够模拟生物神经系统**对真实世界物体所做出的反应**”。

这一章的内容大致如下：

- **神经元模型**：什么是神经元模型？它的构造是怎样的？激活函数是什么？如何组建神经网络？

- **感知机与多层网络**：感知机的构造是怎样的？感知机不能解决什么问题？多层网络是怎样的？多层前馈神经网络有什么特点？

- **误差逆传播算法**：BP算法如何调整参数？有什么值得注意的地方？标准BP算法和累积BP算法有什么区别？如何设置隐层神经元的个数？如何处理BP神经网络的过拟合问题？

- **全局最小与局部极小**：什么是全局最小？什么是局部极小？如何跳出局部极小？

- **其他常见神经网络**：有哪些常见的神经网络？它们各自有什么特点？

- **深度学习**：深度学习是如何提升模型容量的？如何训练深度神经网络？怎么理解深度学习？

## 神经元模型

**神经元（neuron）模型**是神经网络最基本的组成成分，不妨对比一下生物学中的神经元和机器学习中的神经元：

![neuron](http://www.neuralpower.com/images/tecnology/figure1.gif)
![neuron](http://blogs.cornell.edu/info2040/files/2012/11/neuron-model-24yuchi.png)

在生物学中，每个神经元都有多个**树突（dendrite）**，一个**轴突（axon）**，以及一个**细胞体（cell body）**。当它**兴奋**时，就会向相连的神经元发送化学物质，从而改变它们内部的电位。如果一个神经元的电位超过了**阈值（threshold，也称bias）**，那它就会被**激活**，也即兴奋，继而向其他相连神经元发送化学物质。

在机器学习中，最常用的是右图的**M-P神经元模型**（亦称为**阈值逻辑单元（threshold logic unit）**）。树突对应于输入部分，每个神经元接收到若干个来自其他神经元的输入信号，这些信号通过带权重的**连接（connection）**传递给细胞体，这些权重又称为**连接权**。细胞体分为两部分，前一部分计算**总输入值（即输入信号的加权和，或者说累积电平）**；后一部分先计算总输入值与该神经元的阈值的差值，然后通过**激活函数（activation function）**的处理，从轴突输出给其它神经元。也即：

$$y = f(\sum_i w_i x_i -\theta)$$

最理想的激活函数是阶跃函数，但它不连续。类似于线性分类，可以采用Sigmoid函数来近似。因为这类函数能把较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为**挤压函数（squashing function）**。

将**多个神经元按一定的层次结构连接起来**，就得到了神经网络。它是一种包含许多参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值）。若将每个神经元都看作一个函数，整个神经网络就是由这些函数相互嵌套而成。

## 感知机与多层网络

**感知机（Perceptron）**仅由两层神经元组成，如下图：

![perceptron](https://github.com/familyld/Machine_Learning/blob/master/graph/perceptron.png?raw=true)

两层是指输入层和输出层，但**只有输出层是M-P神经元**，也即**只有一层功能神经元（functional neuron）**。输入层只负责把每一个样本的各个属性传递给输出层（**输入层的神经元数量等于样本的属性数目**），不进行函数处理。其实说白了这个模型跟逻辑回归是一样的，不过按我的理解就是感知机的输出层可以有多个神经元，产生多个输出。而且线性模型中偏置项是和属性一起加权求和的，但神经网络中则是求属性加权和和预测的差。

有时候阈值 $\theta$ 可以看作一个**输入固定为 $-1.0$ 的哑结点（dummy node）**，连接权为 $w_{n+1}$。这样就可以把权重和阈值的学习统一为权重的学习了。更新权重的方式如下：

$$w_i \leftarrow w_i + \Delta w_i$$

$$\Delta  w_i= \eta (y - \hat{y}) x_i$$

其中，$\eta$ 称为**学习率（learning rate）**，感知机是**逐个数据点输入来更新**的。设定初始的权重后，逐个点输入，如果没有预测错就继续检验下一个点；如果预测错了就更新权重，然后重新开始逐个点检验，**直到所有点都预测正确了就停止更新**（所以这其实是一种最小化经验误差的方法）。

已经证明了，**若两类模式是线性可分（linearly separable）的，即存在一个线性超平面能将它们分开**。比如二维平面上可以用一条**直线**完全分隔开两个类别的点。由于感知机只有一层功能神经元，所以学习能力极其有限，**只能处理线性可分问题**。对于这类问题，感知机的学习过程必然**收敛（converge）**而求出适当的权向量；对于线性不可分问题，感知机的学习过程会发生**振荡（fluctuation）**，难以稳定下来。

**使用多层的功能神经元可以解决线性不可分问题**，比方说两层（功能神经元）的感知机就可以解决异或问题（在二维平面中需要使用两条直线才能分隔开）。在输入层和输出层之间的层称为隐层或者**隐含层（hidden layer）**，隐含层的神经元也是功能神经元。

![multi-layer feedforward neural networks](https://github.com/familyld/Machine_Learning/blob/master/graph/multi-layer_feedforward_neural_networks.png?raw=true)

上图展示的最为常见的多层神经网络——**多层前馈神经网络（multi-layer feedforward neural networks）**，它有以下特点：

1. 每层神经元与下一层神经元**全互连**
2. 神经元之间**不存在同层连接**
3. 神经元之间**不存在跨层连接**

因为说两层网络时容易有歧义（只包含输入层输出层？还是包含两层功能神经元？），所以一般称包含一个隐含层的神经网络为**单隐层网络**。只要包含隐层，就可以称为**多层网络**。神经网络的学习其实就是调整各神经元之间的**连接权（connection weight）**以及各神经元的阈值，也即是说，**神经网络学到的东西都蕴含在连接权和阈值中**了。



