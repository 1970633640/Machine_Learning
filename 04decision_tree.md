# 决策树

由于决策树的内容我之前有做过一个比较详细的[PPT分享](https://github.com/familyld/Machine_Learning/blob/master/resource/Decision%20tree.pdf)，所以这一章的笔记暂时不打算花太大精力，主要是理清最重要的定义和思路，更详细的之后有时间会考虑补上。

这一章的内容大致如下：

- **基本流程**：决策树是如何决策的？决策树学习的目的是什么？如何生成一颗决策树？

- **划分选择**：怎样选择最优划分属性？有哪些判断指标？具体是怎样运作的？

- **剪枝处理**：为什么要剪枝？如何判断剪枝后决策树模型的泛化性能是否提升？预剪枝和后剪枝是怎样工作的？有什么优缺点？

- **连续与缺失值**：如何把连续属性离散化？如何基于离散化后的属性进行划分？和离散属性有何不同？如何在属性值缺失的情况下选择最优划分属性？给定划分属性，如何划分缺失该属性值的样本？

- **多变量决策树**：决策树模型的分类边界的特点是怎样的？多变量决策数是如何定义的？又是如何工作的？

## 基本流程

**决策树（decision tree）**是一种模仿人类决策的学习方法。举个例子，比方说买电脑，我们首先看看外观帅不帅气，然后再看看性能怎么样，还得看看价格如何，最终**经过一系列的判断做出**是否购买电脑的**决策**。

一棵决策树可以分成三个部分：叶节点，非叶节点，分支。**叶节点**对应**决策结果**，也即分类任务中的类别标记；**非叶节点**（包括根节点）对应一个**判定问题**（某属性=？）；**分支**对应父节点**判定问题的不同答案**（可能的属性值），可能连向一个非叶节点的子节点，也可能连向叶节点。

![decision tree](https://upload.wikimedia.org/wikipedia/en/5/5a/Decision_tree_for_playing_outside.png)

决策就是从根节点开始走到叶节点的过程。每经过一个节点的判定，数据集就按照答案（属性值）划分为若干子集，**在子节点做判定时只需要考虑对应的数据子集就可以了**。

决策树学习的目的是为了**产生一棵泛化能力强，即处理未见示例能力强的决策树**。

决策树生成是一个**递归过程**：

生成算法：

1. 传入训练集和属性集
2. 生成一个新结点
3. 若此时数据集中所有样本都属于同一类，则把新结点设置为该类的叶节点，然后**返回**$^1$。
4. 若此时属性集为空，或者数据集中所有样本在属性集余下的所有属性上取值都相同，无法进一步划分，则把新结点设置为叶节点，类标记为数据集中样本数最多的类，然后**返回**$^2$
5. 从属性集中选择一个最优划分属性
    - 为该属性的每个属性值生成一个分支，并按属性值划分出子数据集
    - 若分支对应的子数据集为空，无法进一步划分，则直接把子节点设置为叶节点，类标记为父节点数据集中样本数最多的类，然后**返回**$^3$
    - 将子数据集和去掉了划分属性的子属性集作为算法的传入参数，继续生成该分支的子决策树。

稍微注意以下，3处返回中的第2处和第3处设置叶节点的类标记原理有所不同。第2处将类标记设置为当前结点对应为数据集中样本数最多的类，这是利用当前结点的**后验分布**；第3处将类标记设置为为父节点数据集中样本数最多的类，这是把父节点的样本分布作为当前结点的**先验分布**。
