# 决策树

由于决策树的内容我之前有做过一个比较详细的[PPT分享](https://github.com/familyld/Machine_Learning/blob/master/resource/Decision%20tree.pdf)，所以这一章的笔记暂时不打算花太大精力，主要是理清最重要的定义和思路，更详细的之后有时间会考虑补上。

这一章的内容大致如下：

- **基本流程**：决策树是如何决策的？决策树学习的目的是什么？如何生成一颗决策树？

- **划分选择**：怎样选择最优划分属性？有哪些判断指标？具体是怎样运作的？

- **剪枝处理**：为什么要剪枝？如何判断剪枝后决策树模型的泛化性能是否提升？预剪枝和后剪枝是怎样工作的？有什么优缺点？

- **连续与缺失值**：如何把连续属性离散化？如何基于离散化后的属性进行划分？和离散属性有何不同？如何在属性值缺失的情况下选择最优划分属性？给定划分属性，如何划分缺失该属性值的样本？

- **多变量决策树**：决策树模型的分类边界的特点是怎样的？多变量决策数是如何定义的？又是如何工作的？

## 基本流程

**决策树（decision tree）**是一种模仿人类决策的学习方法。举个例子，比方说买电脑，我们首先看看外观帅不帅气，然后再看看性能怎么样，还得看看价格如何，最终**经过一系列的判断做出**是否购买电脑的**决策**。

一棵决策树可以分成三个部分：叶节点，非叶节点，分支。**叶节点**对应**决策结果**，也即分类任务中的类别标记；**非叶节点**（包括根节点）对应一个**判定问题**（某属性=？）；**分支**对应父节点**判定问题的不同答案**（可能的属性值），可能连向一个非叶节点的子节点，也可能连向叶节点。

![decision tree](https://upload.wikimedia.org/wikipedia/en/5/5a/Decision_tree_for_playing_outside.png)

决策就是从根节点开始走到叶节点的过程。每经过一个节点的判定，数据集就按照答案（属性值）划分为若干子集，**在子节点做判定时只需要考虑对应的数据子集就可以了**。

决策树学习的目的是为了**产生一棵泛化能力强，即处理未见示例能力强的决策树**。

决策树生成是一个**递归过程**：

生成算法：

1. 传入训练集和属性集
2. 生成一个新节点
3. 若此时数据集中所有样本都属于同一类，则把新节点设置为该类的叶节点，然后**返回**$^1$。
4. 若此时属性集为空，或者数据集中所有样本在属性集余下的所有属性上取值都相同，无法进一步划分，则把新节点设置为叶节点，类标记为数据集中样本数最多的类，然后**返回**$^2$
5. 从属性集中选择一个最优划分属性
    - 为该属性的每个属性值生成一个分支，并按属性值划分出子数据集
    - 若分支对应的子数据集为空，无法进一步划分，则直接把子节点设置为叶节点，类标记为父节点数据集中样本数最多的类，然后**返回**$^3$
    - 将子数据集和去掉了划分属性的子属性集作为算法的传入参数，继续生成该分支的子决策树。

稍微注意以下，3处返回中的第2处和第3处设置叶节点的类标记原理有所不同。第2处将类标记设置为当前节点对应为数据集中样本数最多的类，这是利用当前节点的**后验分布**；第3处将类标记设置为为父节点数据集中样本数最多的类，这是把父节点的样本分布作为当前节点的**先验分布**。

## 划分选择

在决策树模型中，我们不断进行判定的初衷是希望**划分后需要考虑的可能更少**，准确地说，是希望所得子节点的**纯度（purity）**更高（也可以说是混乱程度更低）。

**信息熵（information entropy）**是一种衡量样本集纯度的常用指标：

$$Ent(D) = -\sum_{k=1}^{|\mathcal{Y}|}p_klog_2p_k$$

**一定要记得最前面的负号！！！**其中 $|\mathcal{Y}|$ 为类别集合，$p_k$ 为该类样本占样本总数的比例。

**信息熵越大，表示样本集的混乱程度越高，纯度越低**。

#### 信息增益

**信息增益（information gain）**是**ID3算法**采用的选择准则，定义如下：

$$Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$$

它描述的是按某种属性划分后纯度的提升，**信息增益越大，代表用属性 $a$ 进行划分所获得的纯度提升越大**。其中 $V$ 表示属性 $a$ 的属性值集合，$D^v$ 表示属性值为 $v$ 的数据子集。求和项也称为**条件熵**，我们可以理解为它是先求出每个数据子集的信息熵，然后按每个数据子集占原数据集的比例来赋予权重，比例越大，对提升纯度的帮助就越大。

多个属性都取得最大的信息增益时，任选一个即可。

信息增益又称为**互信息（Mutual information）**。

- 一个连续变量X的不确定性，用方差Var(X)来度量
- 一个离散变量X的不确定性，用熵H(X)来度量
- 两个连续变量X和Y的相关度，用协方差或相关系数来度量
- 两个离散变量X和Y的相关度，用互信息I(X;Y)来度量(直观地，X和Y的相关度越高，X对分类的作用就越大)

#### （信息）增益率

**增益率（gain ratio）**是**C4.5算法**采用的选择准则，定义如下：

$$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中，

$$IV(a) = -\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$$

**一定要记得最前面的负号！！！**IV称为属性的**固有值（intrinsic value）**，它的定义和信息熵是类似的，信息熵衡量的是样本集在类别上的混乱程度，而**固有值衡量的是样本集在某个属性上的混乱程度。固有值越大，则该属性混乱程度越高，可能的取值越多**。

之所以要定义增益率是为了**避免模型过份偏好用取值多的属性作划分**。这是使用信息增益作准则非常容易陷入的误区，比方说每个样本都有一个“编号”属性，这个属性的条件熵肯定是最小的，但如果选择了该属性作为根节点，那么构建出的决策树就没有任何意义了，因为这个模型根本不具备泛化性能。

注意了，**C4.5并非直接选择增益率最高的属性**，它是先从属性集中找到信息增益高于平均水平的属性作为候选，然后再比较这些候选属性的增益率，从中选择增益率最高的。

#### 基尼指数

**基尼指数（Gini index）**是**CART算法**采用的选择准则，定义如下：

基尼值：

$$Gini(D) = \sum_{k=1}^{|\mathcal{Y}|}\sum_{k' \neq k}p_kp_{k'}\\
=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2$$

基尼指数：

$$Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$$

基尼值是另一种衡量样本集纯度的指标。反映的是**从一个数据集中随机抽取两个样本，其类别标志不同的概率**。

**基尼值越小，样本集的纯度越高**。

由基尼值引伸开来的就是基尼指数这种准则了，**基尼指数越小，表示使用属性 $a$ 划分后纯度的提升越大**。

## 剪枝处理

**剪枝（pruning）**是决策树学习算法应对**过拟合**的主要手段。因为决策树模型太强大了，很可能把训练集学得太好以致于把训练集本身的特性也给学习了（特别是属性数多于样本数的情况），所以去除掉一些分支是有必要的。

怎么判断剪枝有没有用呢？具体来说就是**判断剪枝后模型的泛化性能有没有提升**？这就涉及到第二章[模型评估与选择](https://github.com/familyld/Machine_Learning/blob/master/02model_evaluation_and_model_selection.md)的内容了。不过这里不用做比较检验，我们需要做的首先是**选定一种评估方法划分训练集和测试集**，然后**选定一种性能度量**用来衡量剪枝前后的模型在测试集上的效果。

#### 预剪枝

**预剪枝（prepruning）**是在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升（比方说，划分后在测试集上错得更多了 / **划分前后在测试集上效果相同**），就停止划分并将当前节点标记为叶节点。

#### 后剪枝

**后剪枝（postpruning）**是先从训练集生成一颗完整的决策树，然后自底向上地逐个考察非叶节点，若将该节点对应的子树替换为叶节点能带来决策树泛化性能的提升，则将该子树替换为叶节点。实际任务中，即使没有提升，**只要不是性能下降，一般也会剪枝**，因为根据奥卡姆剃刀准则，简单的模型更好。

特别地，只有一层划分（即只有根节点一个非叶节点）的决策树称为**决策树桩（decision stump）**。

#### 优缺点

预剪枝是一种贪心策略，因为它在决策树生成时就杜绝了很多分支展开的机会，所以不但**降低了过拟合的风险**，同时也**显著减少了模型的训练时间开销和测试时间开销**。但是！这种贪心策略有可能导致**欠拟合**，因为有可能当前划分不能提升模型的泛化性能，但其展开的后续划分却会显著提升泛化性能。在预剪枝中这种可能被杜绝了。

后剪枝是种比较保守的策略，**欠拟合的风险很小，泛化性能往往优于预剪枝的决策树**。但是由于后剪枝是在生成了完整决策树后，自底向上对所有非叶节点进行考察，所以**训练时间开销要比未剪枝决策树和预剪枝决策树都大得多**。
