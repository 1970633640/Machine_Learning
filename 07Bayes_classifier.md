# 贝叶斯分类器

**贝叶斯分类器（Bayes Classifier）**是一种通过最大化后验概率进行单点估计的分类器。

这一章的内容大致如下：

- **贝叶斯决策论**：如何计算某个样本误分类的期望损失/条件风险？贝叶斯判定准则是怎样的？什么是判别式模型？什么是生成式模型？贝叶斯定理中各个概率代表什么？估计后验概率有什么难处？

- **极大似然估计**：如何估计类条件概率？频率学派和贝叶斯学派对参数估计有什么不同的见解？极大似然估计的思想是什么？如何处理概率连成造成的下溢？试想一下连续属性和离散属性的极大似然估计。这种估计方法有什么缺点？

- **朴素贝叶斯分类器**：朴素贝叶斯分类器是基于什么假设的？表达式怎么写？为什么估计概率值时需要进行平滑？拉普拉斯修正是怎样的？现实任务中中如何使用朴素贝叶斯分类器？

- **半朴素贝叶斯分类器**：半朴素贝叶斯分类器是基于什么假设的？什么是独依赖估计？独依赖分类器有哪些学习方法？是否可以通过考虑属性之间的高阶依赖来进一步提升模型的泛化性能？

- **贝叶斯网络**：什么是贝叶斯网络？它的结构是怎样的？如何进行模型的学习？如何对新样本进行推断？

- **EM算法**：什么是隐变量？什么是边际似然？EM算法的步骤是怎样的？和梯度下降有什么不同？

## 贝叶斯决策论

**贝叶斯决策论（Bayesian decision theory）**是概率框架下实施决策的基本方法。具体来说，在分类任务中，贝叶斯决策论基于概率和误判损失选择出最优的类别标记。

以多分类任务为例，假设有 $N$ 种标记，即 $\mathcal{Y} = {c_1, c_2,..., c_N}$，用 $\lambda_{ij}$ 表示把一个真实标记为 $c_i$ 的样本误分类为 $c_j$ 所产生的损失。那么将样本 $\mathbf{x}$ 分类为 $c_i$ 的**期望损失（expected loss）**或者说，在样本 $\mathbf{x}$ s上的**条件风险（conditional risk）**：

$$ R(c_i | \mathbf{x}) = \sum_{j=1}^N \lambda_{ij} P(c_j | \mathbf{x})$$

它描述的是，给定一个样本 $\mathbf{x}$，把它分类为 $c_i$ 需要冒多大的风险。或者说，当样本真实标记不是 $c_i$ 时，会有多大的损失。这个损失是一个求和，每一个求和项都是某一类别的后验概率和对应误分类损失的积。（**注**：书中这个地方不够细致，求和项的下标是要排除 $i$ 本身的）

在单个样本条件风险的基础上，可以定义**总体风险**：

$$ R(h) = \mathbb{E}_{\mathbf{x}}[R(h(\mathbf{x})\ |\ \mathbf{x})]$$

它描述的是，**所有样本的条件风险的数学期望**。其中 $h$ 是一种用于产生分类结果的判断准则。

那么我们的目标就是找出能最小化总体风险 $R(h)$ 的判断准则。怎样的判断准则能满足这个要求呢？很直观地，如果一个判断准则 $h$ 能最小化所有样本 $\mathbf{x}$ 的条件风险，那它对应的总体风险必然也是最小的。由此，可以得到**贝叶斯判定准则（Bayes decision rule）**：要最小化总体风险，只需**在每个样本上选择能使对应的条件风险 $R(c\ |\ \mathbf{x})$ 最小的标记**。即：

$$h^*(\mathbf{x}) = \arg \min_{c \in \mathcal{Y}} R(c\ |\ \mathbf{x})$$

这个判断准则 $h^*$ 称为**贝叶斯最优分类器（Bayes optimal classifier）**，对应的总体风险 $R(h^*)$ 称为**贝叶斯风险（Bayes risk）**，而  $1-R(h^*)$ 则反映了分类器所能达到的最好性能，也即**模型精度的理论上限**。

进一步地，如果我们学习模型的目标是**令分类错误率最小**，那么分类正确时误分类损失 $\lambda_{ij}$ 为0，反之为1。这是条件风险就是：

$$R(c\ |\ \mathbf{x}) = 1-P(c\ |\ \mathbf{x})$$

要令风险最小，我们只需要选择使样本 $\mathbf{x}$ 后验概率最大的一个类别标记就可以了。

问题在于，**怎样获取后验概率呢？**

事实上，从概率的角度来理解，机器学习的目标就是**基于有限的训练样本集尽可能准确地估计出后验概率**（当然，大多数机器学习技术无需准确估计出后验概率）。要实现这个目标，主要有两种策略：

- 构建**判别式模型（discriminative models）**：给定样本 $\mathbf{x}$，直接对后验概率 $P(\mathbf{x}\ |\ c)$ 建模来预测 $c$。这类模型包括决策树、BP神经网络、支持向量机等等。

- 构建**生成式模型（generative models）** ：给定样本 $\mathbf{x}$，先对联合概率分布 $P(\mathbf{x},c)$ 建模，然后再利用联合概率计算出后验概率 $P(c\ |\ \mathbf{x})$，也即 $P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x},c)}{P(\mathbf{x})}$。

又因为联合概率 $P(\mathbf{x},c) = P(c\ |\ \mathbf{x}) \times P(\mathbf{x}) = P(\mathbf{x}\ |\ c) \times P(c)$，由此，能得到**贝叶斯定理**：

$$P(c\ |\ \mathbf{x}) = \frac{P(\mathbf{x}\ |\ c) \times P(c)}{P(\mathbf{x})}$$

在贝叶斯定理中，每个概率都有约定俗成的名称：

- $P(c\ |\ \mathbf{x})$ 是类标记 $c$ 相对于样本 $\mathbf{x}$ 的条件概率，也由于得自 $\mathbf{x}$ 的取值而被称作 $c$ 的后验概率。

- $P(\mathbf{x}\ |\ c)$ 是样本 $\mathbf{x}$ 相对于类标记 $c$ 的**类条件概率（class-conditional probability）**，或称为**似然（likelihood）**，也由于得自 $c$ 的取值而被称作 $\mathbf{x}$ 的后验概率。

- $P(c)$ 是 $c$ 的先验概率（也称为边缘概率），之所以称为"先验"是因为它不考虑任何 $\mathbf{x}$ 方面的因素。在这里又称为**类先验（prior）概率**。

- $P(\mathbf{x})$ 是 $\mathbf{x}$ 的先验概率。在这里是用作归一化的**证据（evidence）因子**，与类标记无关。

有了贝叶斯定理，如何估计后验概率 $P(c\ |\ \mathbf{x})$ 的问题就转化为**如何计算类先验概率 $P(c)$ 和类条件概率 $P(\mathbf{x}\ |\ c)$ **了。

类先验概率 $P(c)$ 表示的是**样本空间中各类样本的比例**，根据大数定律，**当训练集包含足够多的独立同分布样本**时，类先验概率可以直接通过**训练集中各类样本出现的频率**进行估计。

类条件概率 $P(\mathbf{x}\ |\ c)$的情况就复杂多了，它涉及到类 $c$ 中**样本 $\mathbf{x}$ 所有属性的联合概率**，假设每个样本有 $d$ 个二值属性，那么可能的取值组合就多达 $2^d$ 个，这个数目可能**远多于训练集的规模**，也就意味着很多样本的取值没有在训练集中出现，所以**直接用训练集出现的频率进行估计是不可行的**。必须注意**未被观测到**和**出现概率为0**的区别。

**注意**，上述讨论中，均假设属性是离散型，对于连续型属性，只需把概率质量函数 $P(\cdot)$ 换为概率密度函数 $p(\cdot)$ 就可以了。

## 极大似然估计

估计类条件概率的一种常用策略是：先**假定该类样本服从某种确定的概率分布形式**，然后再**基于训练集中的该类样本对假定的概率分布的参数进行估计**。比方说假定该类样本服从高斯分布，那么接下来就是利用训练集中该类样本来估计高斯分布的参数——均值和方差。

具体来说，如果类 $c$ 的样本服从参数为 $\theta_c$（可能不止一个参数）的分布，那么我们从样本空间抽取到该类的某一个样本 $\mathbf{x}$ 的概率就是 $P(\mathbf{x}\ |\ \theta_c)$。使用 $D_c$ 来表示训练集中类 $c$ 的子集，可以定义数据集 $D_c$ 的**似然（likelihood）**为：

$$P(D_c\ |\ \theta_c) = \prod_{\mathbf{x} \in D_c} P(\mathbf{x}\ |\ \theta_c)$$

由于**连乘操作容易造成下溢**，实际任务中通常使用**对数似然（log-likelihood）**代替：

$$LL(\theta_c) = \log P(D_c\ |\ \theta_c) = \sum_{\mathbf{x} \in D_c} \log P(\mathbf{x}\ |\ \theta_c)$$

所谓**极大似然估计（Maximum Likelihood Estimation，简称MLE）**就是**找出令似然最大的参数 $\theta_c$**。也即从 $\theta_c$ 的所有可能取值中找到一个**令所抽取样本出现的可能性最大**的值。

求解的过程也很简单，就是求似然函数的导数，令导数为0，得到**似然方程**，解似然方程得到最优解，也即该类样本分布的参数。

特别地，对于参数估计，**频率主义学派（Frequentist）**和**贝叶斯学派（Bayesian）**有不同的见解。前者认为，参数虽然未知，却是**客观存在的固定值**，因此可以用优化似然函数等准则确定参数值；后者认为，参数是**未观测到的随机变量**，**参数本身也存在分布**。所以可以先假定参数服从一个先验分布，然后再根据观测到的数据计算参数的后验分布。这一节讨论的极大似然估计方法源于频率主义学派。

尽管极大似然估计能使我们求取类条件概率的过程变得相对简单，但它有最大的一个缺点就是：估计结果的**准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布**。在实际任务中，我们需要利用任务所属领域的一些经验知识，全凭猜测是很容易产生误导性结果的。

## 习题

#### 7.1

> 问：试使用极大似然法估算西瓜数据集3.0中前3个属性的类条件概率。

#### 7.2*

> 问：试证明：条件独立性假设不成立时，朴素贝叶斯分类器仍有可能产生最优贝叶斯分类器。

#### 7.3

> 问：试编程实现拉普拉斯修正的朴素贝叶斯分类器，并以西瓜数据集3.0为训练集，对 page.151 的 “测1” 样本进行判别。

#### 7.4

> 问：实践中使用式（7.15）决定分类类别时，若数据的维数非常高，则概率连乘 $\prod_{i=1}^d P(x_i | c)$ 的结果通常会非常接近于0从而导致下溢。试述防止下溢的可能方案。

#### 7.5

> 问：试证明：二分类任务中两类数据满足高斯分布且方差相同时，线性判别分析产生贝叶斯最优分类器。

#### 7.6

> 问：试编程实现AODE分类器，并以西瓜数据集3.0为训练集，对 page.151 的 “测1” 样本进行判别。

#### 7.7

> 问：给定d个二值属性的二分类任务，假设对于任何先验概率项的估算至少需30个样例，则在朴素贝叶斯分类器式（7.15）中估算先验概率项 $P(c)$ 需 $30 \times 2 =60$ 个样例。试估计在AODE式（7.23）中估算先验概率项 $P(c, x_i)$ 所需的样例数（分别考虑最好和最坏情形）。

#### 7.8

> 问：考虑图7.3，试证明：在同父结构中，若 $x_1$ 的取值未知，则 $x_3\ ㅛ\ x_4$ 不成立；在顺序结构中，$y\ ㅗ\ z | x$，但 $y\ ㅛ\ z$ 不成立。

#### 7.9

> 问：以西瓜数据集2.0为训练集，试基于BIC准则构建一个贝叶斯网。

#### 7.10

> 问：以西瓜数据集2.0中属性 “脐部” 为隐变量，试基于EM算法构建一个贝叶斯网。

